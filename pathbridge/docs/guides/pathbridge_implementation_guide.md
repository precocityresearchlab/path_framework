---
created_date: 2025-01-27
created_by: PATH Framework Team
last_modified: 2025-01-27
version: 1.0.0
purpose: Complete implementation guide for PathBridge AI Coding Assistant
framework_phase: N/A
dependencies: [pathbridge_ai_coding_assistant.md, CoreAgent, PATH Framework]
status: active
tags: [pathbridge, implementation, architecture, development, how-to]
---

# PathBridge AI Coding Assistant - Implementation Guide

![Framework](https://img.shields.io/badge/Framework-PATH-orange?style=flat-square)
![Version](https://img.shields.io/badge/Version-1.0.0-blue?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)
![Status](https://img.shields.io/badge/Status-Active-brightgreen?style=flat-square)
![Institution](https://img.shields.io/badge/Institution-PATH%20Framework-purple?style=flat-square)
![Methodology](https://img.shields.io/badge/Methodology-Implementation%20Guide-red?style=flat-square)

## Table of Contents

- [Overview](#overview)
- [Architecture & Design](#architecture--design)
  - [System Architecture](#system-architecture)
  - [Component Interaction Flow](#component-interaction-flow)
  - [Database Schema Design](#database-schema-design)
- [Implementation Details](#implementation-details)
  - [Core Agent Implementation](#core-agent-implementation)
  - [Domain Analyst Agent Implementation](#domain-analyst-agent-implementation)
  - [Agent Orchestrator Implementation](#agent-orchestrator-implementation)
- [Development Workflow](#development-workflow)
  - [Step-by-Step Build Process](#step-by-step-build-process)
  - [Testing Strategies](#testing-strategies)
  - [Performance Optimization Techniques](#performance-optimization-techniques)
- [Deployment Procedures](#deployment-procedures)
  - [Docker Configuration](#docker-configuration)
  - [Kubernetes Deployment](#kubernetes-deployment)
- [Monitoring Setup](#monitoring-setup)
  - [Application Metrics](#application-metrics)
  - [Logging Configuration](#logging-configuration)
- [Security Implementation](#security-implementation)
  - [Authentication & Authorization](#authentication--authorization)
  - [Input Validation & Sanitization](#input-validation--sanitization)
- [Error Handling Patterns](#error-handling-patterns)
  - [Comprehensive Error Handling](#comprehensive-error-handling)
  - [Retry Logic Implementation](#retry-logic-implementation)
- [Configuration Management](#configuration-management)
  - [Environment-Based Configuration](#environment-based-configuration)
  - [Feature Flags System](#feature-flags-system)
- [Integration Examples](#integration-examples)
  - [LLM Provider Integration](#llm-provider-integration)
  - [RAG System Integration](#rag-system-integration)
  - [Testing Framework Integration](#testing-framework-integration)
  - [File Operations Integration](#file-operations-integration)
  - [Human Validation Integration](#human-validation-integration)
  - [Security Integration](#security-integration)
  - [IDE Plugin Integration](#ide-plugin-integration)
  - [CI/CD Pipeline Integration](#cicd-pipeline-integration)
- [Performance Metrics & Monitoring](#performance-metrics--monitoring)
  - [Business Metrics Dashboard](#business-metrics-dashboard)
  - [Quality Metrics Tracking](#quality-metrics-tracking)
- [Troubleshooting Guide](#troubleshooting-guide)
  - [Common Issues & Solutions](#common-issues--solutions)
- [Best Practices](#best-practices)
  - [Development Best Practices](#development-best-practices)
  - [Deployment Best Practices](#deployment-best-practices)
- [Getting Started](#getting-started)
  - [Quick Setup Guide](#quick-setup-guide)
  - [First User Story Example](#first-user-story-example)

## Overview

This guide provides complete implementation details for building the PathBridge AI Coding Assistant, covering architecture, design patterns, code examples, and deployment strategies.

## Architecture & Design

### **System Architecture**

```mermaid
graph TB
    subgraph "User Interface"
        UI[IDE/API/CLI]
    end
    
    subgraph "Core System"
        O[Simple Orchestrator]
        A[AI Agent Pool]
    end
    
    subgraph "Services"
        LLM[LLM Client]
        FS[File System]
        SEC[Security Scanner]
        HV[Human Validator]
    end
    
    subgraph "Storage"
        DB[(Database)]
        CACHE[(Cache)]
    end
    
    UI --> O
    O --> A
    A --> LLM
    A --> FS
    A --> SEC
    O --> HV
    LLM --> CACHE
    O --> DB
```

### **Simplified Interaction Flow**

```mermaid
sequenceDiagram
    participant User
    participant Orchestrator
    participant Agent
    participant LLM
    participant FileSystem
    
    User->>Orchestrator: Submit User Story
    
    loop For Each Task
        Orchestrator->>Agent: Execute Task
        Agent->>LLM: Generate Solution
        LLM->>Agent: Response
        Agent->>FileSystem: Write Files
        Agent->>Orchestrator: Task Complete
    end
    
    Orchestrator->>User: Feature Delivered
```

### **Database Schema Design**

```sql
-- Core Tables
CREATE TABLE user_stories (
    id UUID PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    acceptance_criteria JSONB NOT NULL,
    business_value TEXT,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE agent_executions (
    id UUID PRIMARY KEY,
    story_id UUID REFERENCES user_stories(id),
    agent_type VARCHAR(100) NOT NULL,
    phase INTEGER NOT NULL,
    input_data JSONB,
    output_data JSONB,
    status VARCHAR(50) DEFAULT 'running',
    started_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,
    execution_time_ms INTEGER
);

CREATE TABLE human_validations (
    id UUID PRIMARY KEY,
    story_id UUID REFERENCES user_stories(id),
    decision_type VARCHAR(100) NOT NULL,
    options JSONB NOT NULL,
    recommendation TEXT,
    decision JSONB,
    approver_id VARCHAR(100),
    requested_at TIMESTAMP DEFAULT NOW(),
    decided_at TIMESTAMP
);

CREATE TABLE knowledge_base (
    id UUID PRIMARY KEY,
    content_type VARCHAR(100) NOT NULL,
    content_data JSONB NOT NULL,
    embeddings VECTOR(1536),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_user_stories_status ON user_stories(status);
CREATE INDEX idx_agent_executions_story_phase ON agent_executions(story_id, phase);
CREATE INDEX idx_human_validations_pending ON human_validations(requested_at) WHERE decided_at IS NULL;
CREATE INDEX idx_knowledge_base_embeddings ON knowledge_base USING ivfflat (embeddings vector_cosine_ops);
```

## Implementation Details

### **Core Agent Implementation**

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio
import logging

@dataclass
class CapabilityRequest:
    """Request structure for agent capabilities"""
    capability_name: str
    parameters: Dict[str, Any]
    context: Dict[str, Any]
    user_story_id: str
    requester_agent: Optional[str] = None

@dataclass
class CapabilityResponse:
    """Response structure from agent capabilities"""
    success: bool
    result: Dict[str, Any]
    errors: List[str]
    warnings: List[str]
    execution_time_ms: int
    metadata: Dict[str, Any]

class CoreAgent(ABC):
    """Base class for all PathBridge AI agents"""
    
    def __init__(self, agent_id: str, phase: int, capabilities: List[str]):
        self.agent_id = agent_id
        self.phase = phase
        self.capabilities = capabilities
        self.logger = logging.getLogger(f"agent.{agent_id}")
        self.knowledge_base = None  # Injected by orchestrator
        self.llm_client = None      # Injected by orchestrator
        
    async def execute_capability(self, request: CapabilityRequest) -> CapabilityResponse:
        """Execute a specific capability with full error handling"""
        start_time = datetime.now()
        
        try:
            # Validate request
            if not await self._validate_request(request):
                return CapabilityResponse(
                    success=False,
                    result={},
                    errors=[f"Invalid request for capability: {request.capability_name}"],
                    warnings=[],
                    execution_time_ms=0,
                    metadata={"agent_id": self.agent_id}
                )
            
            # Execute capability
            result = await self._execute_capability_internal(request)
            
            # Store results in knowledge base
            await self._store_execution_result(request, result)
            
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return CapabilityResponse(
                success=True,
                result=result,
                errors=[],
                warnings=[],
                execution_time_ms=int(execution_time),
                metadata={
                    "agent_id": self.agent_id,
                    "phase": self.phase,
                    "capability": request.capability_name
                }
            )
            
        except Exception as e:
            self.logger.error(f"Capability execution failed: {str(e)}")
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return CapabilityResponse(
                success=False,
                result={},
                errors=[str(e)],
                warnings=[],
                execution_time_ms=int(execution_time),
                metadata={"agent_id": self.agent_id}
            )
    
    @abstractmethod
    async def _execute_capability_internal(self, request: CapabilityRequest) -> Dict[str, Any]:
        """Internal capability execution - implemented by subclasses"""
        pass
    
    async def _validate_request(self, request: CapabilityRequest) -> bool:
        """Validate capability request"""
        if request.capability_name not in self.capabilities:
            return False
        
        # Add specific validation logic here
        return True
    
    async def _store_execution_result(self, request: CapabilityRequest, result: Dict[str, Any]):
        """Store execution result in knowledge base"""
        if self.knowledge_base:
            await self.knowledge_base.store_agent_output(
                agent_id=self.agent_id,
                operation=request.capability_name,
                result=result,
                context=request.context
            )
```

### **Domain Analyst Agent Implementation**

```python
class DomainAnalystAgent(CoreAgent):
    """Phase 1: Domain Analysis and Requirements Processing"""
    
    def __init__(self):
        super().__init__(
            agent_id="domain_analyst",
            phase=1,
            capabilities=[
                "analyze_user_story",
                "generate_domain_model",
                "identify_missing_requirements",
                "validate_business_value"
            ]
        )
    
    async def _execute_capability_internal(self, request: CapabilityRequest) -> Dict[str, Any]:
        """Execute domain analysis capabilities"""
        capability = request.capability_name
        
        if capability == "analyze_user_story":
            return await self._analyze_user_story(request.parameters)
        elif capability == "generate_domain_model":
            return await self._generate_domain_model(request.parameters)
        elif capability == "identify_missing_requirements":
            return await self._identify_missing_requirements(request.parameters)
        elif capability == "validate_business_value":
            return await self._validate_business_value(request.parameters)
        else:
            raise ValueError(f"Unknown capability: {capability}")
    
    async def _analyze_user_story(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze user story for domain patterns and business logic"""
        user_story = params.get("user_story", {})
        
        # Use LLM to analyze the story
        analysis_prompt = f"""
        Analyze this user story for domain patterns and business logic:
        
        Title: {user_story.get('title', '')}
        Description: {user_story.get('description', '')}
        Acceptance Criteria: {user_story.get('acceptance_criteria', [])}
        
        Identify:
        1. Core domain entities
        2. Business rules and constraints
        3. Data relationships
        4. External system dependencies
        5. Edge cases and error scenarios
        
        Return structured analysis in JSON format.
        """
        
        llm_response = await self.llm_client.generate_completion(
            prompt=analysis_prompt,
            temperature=0.1,
            max_tokens=2000
        )
        
        # Parse and validate LLM response
        analysis = self._parse_domain_analysis(llm_response)
        
        return {
            "domain_entities": analysis.get("entities", []),
            "business_rules": analysis.get("rules", []),
            "relationships": analysis.get("relationships", []),
            "dependencies": analysis.get("dependencies", []),
            "edge_cases": analysis.get("edge_cases", []),
            "confidence_score": analysis.get("confidence", 0.8)
        }
    
    async def _generate_domain_model(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Generate entity-relationship models and business rules"""
        domain_analysis = params.get("domain_analysis", {})
        
        # Create domain model structure
        domain_model = {
            "entities": [],
            "value_objects": [],
            "aggregates": [],
            "domain_services": [],
            "repositories": []
        }
        
        # Generate entities from analysis
        for entity_data in domain_analysis.get("domain_entities", []):
            entity = {
                "name": entity_data.get("name"),
                "attributes": entity_data.get("attributes", []),
                "methods": entity_data.get("methods", []),
                "relationships": entity_data.get("relationships", []),
                "business_rules": entity_data.get("rules", [])
            }
            domain_model["entities"].append(entity)
        
        return domain_model
    
    def _parse_domain_analysis(self, llm_response: str) -> Dict[str, Any]:
        """Parse and validate LLM domain analysis response"""
        try:
            import json
            return json.loads(llm_response)
        except json.JSONDecodeError:
            # Fallback parsing logic
            return {
                "entities": [],
                "rules": [],
                "relationships": [],
                "dependencies": [],
                "edge_cases": [],
                "confidence": 0.5
            }
```

### **Agent Orchestrator Implementation**

```python
class AgentOrchestrator:
    """Main orchestrator for coordinating AI agents across all phases"""
    
    def __init__(self):
        self.agents = {}
        self.knowledge_base = SharedKnowledgeBase()
        self.human_validator = HumanValidationEngine()
        self.workflow_engine = WorkflowEngine()
        self.logger = logging.getLogger("orchestrator")
        
    def register_agent(self, agent: CoreAgent):
        """Register an agent with the orchestrator"""
        agent.knowledge_base = self.knowledge_base
        agent.llm_client = self._get_llm_client()
        self.agents[agent.agent_id] = agent
        
    async def execute_user_story(self, user_story: Dict[str, Any]) -> Dict[str, Any]:
        """Execute complete user story through all 4 phases"""
        story_id = user_story.get("id")
        
        try:
            # Phase 1: Software Engineering
            phase1_result = await self._execute_phase_1(story_id, user_story)
            
            # Human validation checkpoint
            architecture_approval = await self.human_validator.request_validation(
                decision_type="architecture_approval",
                options=phase1_result.get("architecture_options", []),
                recommendation=phase1_result.get("recommended_architecture"),
                story_id=story_id
            )
            
            if not architecture_approval.get("approved"):
                return {"success": False, "reason": "Architecture not approved"}
            
            # Phase 2: Test-Driven Development
            phase2_result = await self._execute_phase_2(story_id, phase1_result)
            
            # Phase 3: DevOps & Production
            phase3_result = await self._execute_phase_3(story_id, phase2_result)
            
            # Phase 4: Operations
            phase4_result = await self._execute_phase_4(story_id, phase3_result)
            
            return {
                "success": True,
                "story_id": story_id,
                "phase_results": {
                    "phase_1": phase1_result,
                    "phase_2": phase2_result,
                    "phase_3": phase3_result,
                    "phase_4": phase4_result
                },
                "artifacts": self._collect_artifacts(story_id),
                "metrics": self._calculate_metrics(story_id)
            }
            
        except Exception as e:
            self.logger.error(f"User story execution failed: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _execute_phase_1(self, story_id: str, user_story: Dict[str, Any]) -> Dict[str, Any]:
        """Execute Phase 1: Software Engineering agents"""
        
        # Domain Analysis
        domain_request = CapabilityRequest(
            capability_name="analyze_user_story",
            parameters={"user_story": user_story},
            context={"phase": 1, "story_id": story_id},
            user_story_id=story_id
        )
        
        domain_response = await self.agents["domain_analyst"].execute_capability(domain_request)
        
        if not domain_response.success:
            raise Exception(f"Domain analysis failed: {domain_response.errors}")
        
        # System Architecture
        arch_request = CapabilityRequest(
            capability_name="design_system_architecture",
            parameters={"domain_model": domain_response.result},
            context={"phase": 1, "story_id": story_id},
            user_story_id=story_id
        )
        
        arch_response = await self.agents["system_architect"].execute_capability(arch_request)
        
        return {
            "domain_analysis": domain_response.result,
            "architecture_design": arch_response.result,
            "architecture_options": arch_response.result.get("options", []),
            "recommended_architecture": arch_response.result.get("recommendation")
        }
    
    def _get_llm_client(self):
        """Get configured LLM client"""
        # Implementation depends on chosen LLM provider
        pass
```

## Development Workflow

### **Step-by-Step Build Process**

#### **Phase 1: Foundation Setup (Weeks 1-4)**

```bash
# 1. Project Structure Setup
mkdir pathbridge-ai-assistant
cd pathbridge-ai-assistant

# Create directory structure
mkdir -p {src/{core,agents,orchestration,services},tests,docs,config,scripts}

# 2. Core Dependencies
cat > pyproject.toml << EOF
[project]
name = "pathbridge-ai-assistant"
version = "0.1.0"
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn>=0.24.0",
    "sqlalchemy>=2.0.0",
    "alembic>=1.12.0",
    "asyncpg>=0.29.0",
    "redis>=5.0.0",
    "openai>=1.3.0",
    "anthropic>=0.7.0",
    "langchain>=0.0.350",
    "pgvector>=0.2.0",
    "pydantic>=2.5.0",
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0"
]
EOF

# 3. Install dependencies
uv sync

# 4. Database setup
docker-compose up -d postgres redis

# 5. Run migrations
alembic upgrade head
```

#### **Phase 2: Core Agent Development (Weeks 5-12)**

```python
# src/core/base_agent.py - Implement CoreAgent base class
# src/agents/domain_analyst.py - Implement DomainAnalystAgent
# src/agents/system_architect.py - Implement SystemArchitectAgent
# src/orchestration/agent_orchestrator.py - Implement AgentOrchestrator

# Development workflow for each agent:
# 1. Write failing tests
# 2. Implement minimal functionality
# 3. Refactor and optimize
# 4. Integration testing
```

#### **Phase 3: Integration & Testing (Weeks 13-16)**

```python
# tests/test_integration.py
import pytest
from pathbridge.orchestration.agent_orchestrator import AgentOrchestrator

@pytest.mark.asyncio
async def test_complete_user_story_execution():
    """Test complete user story execution through all phases"""
    orchestrator = AgentOrchestrator()
    
    user_story = {
        "id": "story-001",
        "title": "User Authentication API",
        "description": "As a developer, I want to create a user authentication API...",
        "acceptance_criteria": [
            "Given a valid username and password, when I call /login, then I receive a JWT token",
            "Given an invalid username, when I call /login, then I receive a 401 error"
        ]
    }
    
    result = await orchestrator.execute_user_story(user_story)
    
    assert result["success"] == True
    assert "phase_results" in result
    assert len(result["artifacts"]) > 0
```

### **Testing Strategies**

#### **Unit Testing Pattern**

```python
# tests/agents/test_domain_analyst.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from pathbridge.agents.domain_analyst import DomainAnalystAgent
from pathbridge.core.base_agent import CapabilityRequest

@pytest.fixture
def domain_analyst():
    agent = DomainAnalystAgent()
    agent.llm_client = AsyncMock()
    agent.knowledge_base = AsyncMock()
    return agent

@pytest.mark.asyncio
async def test_analyze_user_story_success(domain_analyst):
    """Test successful user story analysis"""
    # Arrange
    request = CapabilityRequest(
        capability_name="analyze_user_story",
        parameters={
            "user_story": {
                "title": "User Login",
                "description": "As a user, I want to login...",
                "acceptance_criteria": ["Given valid credentials..."]
            }
        },
        context={"story_id": "test-001"},
        user_story_id="test-001"
    )
    
    domain_analyst.llm_client.generate_completion.return_value = '''
    {
        "entities": [{"name": "User", "attributes": ["username", "password"]}],
        "rules": ["Password must be at least 8 characters"],
        "relationships": [],
        "dependencies": ["Authentication Service"],
        "edge_cases": ["Invalid credentials", "Account locked"],
        "confidence": 0.9
    }
    '''
    
    # Act
    response = await domain_analyst.execute_capability(request)
    
    # Assert
    assert response.success == True
    assert "domain_entities" in response.result
    assert len(response.result["domain_entities"]) > 0
    assert response.result["confidence_score"] == 0.9
```

#### **Integration Testing Pattern**

```python
# tests/integration/test_phase_execution.py
@pytest.mark.asyncio
async def test_phase_1_execution():
    """Test complete Phase 1 execution with real agents"""
    orchestrator = AgentOrchestrator()
    
    # Register real agents (not mocks)
    orchestrator.register_agent(DomainAnalystAgent())
    orchestrator.register_agent(SystemArchitectAgent())
    
    user_story = create_test_user_story()
    
    result = await orchestrator._execute_phase_1("test-001", user_story)
    
    assert "domain_analysis" in result
    assert "architecture_design" in result
    assert result["domain_analysis"]["confidence_score"] > 0.7
```

### **Performance Optimization Techniques**

#### **Async Processing Pattern**

```python
class OptimizedAgentOrchestrator:
    """Optimized orchestrator with parallel processing"""
    
    async def execute_parallel_agents(self, agent_tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Execute multiple agents in parallel for better performance"""
        
        # Create tasks for parallel execution
        tasks = []
        for task in agent_tasks:
            agent = self.agents[task["agent_id"]]
            request = CapabilityRequest(**task["request"])
            tasks.append(agent.execute_capability(request))
        
        # Execute all tasks concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results and handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"Agent task {i} failed: {str(result)}")
                processed_results.append({
                    "success": False,
                    "error": str(result),
                    "agent_id": agent_tasks[i]["agent_id"]
                })
            else:
                processed_results.append(result)
        
        return processed_results
```

#### **Caching Strategy**

```python
from functools import wraps
import hashlib
import json

def cache_agent_result(ttl_seconds: int = 3600):
    """Decorator to cache agent execution results"""
    def decorator(func):
        @wraps(func)
        async def wrapper(self, request: CapabilityRequest):
            # Generate cache key
            cache_key = self._generate_cache_key(request)
            
            # Try to get from cache
            cached_result = await self.cache_client.get(cache_key)
            if cached_result:
                return CapabilityResponse(**json.loads(cached_result))
            
            # Execute function
            result = await func(self, request)
            
            # Cache successful results
            if result.success:
                await self.cache_client.setex(
                    cache_key,
                    ttl_seconds,
                    json.dumps(result.__dict__)
                )
            
            return result
        return wrapper
    return decorator

class CachedDomainAnalystAgent(DomainAnalystAgent):
    @cache_agent_result(ttl_seconds=1800)  # 30 minutes
    async def _analyze_user_story(self, params: Dict[str, Any]) -> Dict[str, Any]:
        return await super()._analyze_user_story(params)
```

## Deployment Procedures

### **Docker Configuration**

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY pyproject.toml .
RUN pip install uv && uv sync

# Copy application code
COPY src/ ./src/
COPY config/ ./config/

# Set environment variables
ENV PYTHONPATH=/app/src
ENV ENVIRONMENT=production

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start application
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  pathbridge-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/pathbridge
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./config:/app/config
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg15
    environment:
      - POSTGRES_DB=pathbridge
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

### **Kubernetes Deployment**

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pathbridge-api
  labels:
    app: pathbridge-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: pathbridge-api
  template:
    metadata:
      labels:
        app: pathbridge-api
    spec:
      containers:
      - name: pathbridge-api
        image: pathbridge/ai-assistant:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: pathbridge-secrets
              key: database-url
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: pathbridge-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: pathbridge-api-service
spec:
  selector:
    app: pathbridge-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer
```

## Monitoring Setup

### **Application Metrics**

```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time

# Define metrics
agent_executions_total = Counter(
    'pathbridge_agent_executions_total',
    'Total number of agent executions',
    ['agent_id', 'capability', 'status']
)

agent_execution_duration = Histogram(
    'pathbridge_agent_execution_duration_seconds',
    'Time spent executing agent capabilities',
    ['agent_id', 'capability']
)

active_user_stories = Gauge(
    'pathbridge_active_user_stories',
    'Number of user stories currently being processed'
)

class MetricsCollector:
    """Collects and exports application metrics"""
    
    def record_agent_execution(self, agent_id: str, capability: str, 
                             duration: float, success: bool):
        """Record agent execution metrics"""
        status = 'success' if success else 'failure'
        
        agent_executions_total.labels(
            agent_id=agent_id,
            capability=capability,
            status=status
        ).inc()
        
        agent_execution_duration.labels(
            agent_id=agent_id,
            capability=capability
        ).observe(duration)
    
    def update_active_stories(self, count: int):
        """Update active user stories gauge"""
        active_user_stories.set(count)
```

### **Logging Configuration**

```python
# src/monitoring/logging_config.py
import logging
import json
from datetime import datetime

class StructuredFormatter(logging.Formatter):
    """JSON structured logging formatter"""
    
    def format(self, record):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        # Add extra fields if present
        if hasattr(record, 'user_story_id'):
            log_entry['user_story_id'] = record.user_story_id
        if hasattr(record, 'agent_id'):
            log_entry['agent_id'] = record.agent_id
        if hasattr(record, 'execution_time'):
            log_entry['execution_time_ms'] = record.execution_time
            
        return json.dumps(log_entry)

def setup_logging():
    """Configure structured logging"""
    logging.basicConfig(
        level=logging.INFO,
        handlers=[logging.StreamHandler()],
        format='%(message)s'
    )
    
    # Set formatter for all handlers
    for handler in logging.root.handlers:
        handler.setFormatter(StructuredFormatter())
```

## Security Implementation

### **Authentication & Authorization**

```python
# src/security/auth.py
from fastapi import HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from datetime import datetime, timedelta

security = HTTPBearer()

class AuthManager:
    """Handles authentication and authorization"""
    
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
        self.algorithm = "HS256"
    
    def create_access_token(self, user_id: str, permissions: List[str]) -> str:
        """Create JWT access token"""
        payload = {
            "user_id": user_id,
            "permissions": permissions,
            "exp": datetime.utcnow() + timedelta(hours=24),
            "iat": datetime.utcnow()
        }
        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
    
    def verify_token(self, token: str) -> Dict[str, Any]:
        """Verify and decode JWT token"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            return payload
        except jwt.ExpiredSignatureError:
            raise HTTPException(status_code=401, detail="Token expired")
        except jwt.InvalidTokenError:
            raise HTTPException(status_code=401, detail="Invalid token")

def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Dependency to get current authenticated user"""
    auth_manager = AuthManager(os.getenv("JWT_SECRET_KEY"))
    return auth_manager.verify_token(credentials.credentials)

def require_permission(permission: str):
    """Decorator to require specific permission"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            user = kwargs.get('current_user')
            if not user or permission not in user.get('permissions', []):
                raise HTTPException(status_code=403, detail="Insufficient permissions")
            return await func(*args, **kwargs)
        return wrapper
    return decorator
```

### **Input Validation & Sanitization**

```python
# src/security/validation.py
from pydantic import BaseModel, validator
import re
from typing import List, Optional

class UserStoryRequest(BaseModel):
    """Validated user story request model"""
    title: str
    description: str
    acceptance_criteria: List[str]
    business_value: Optional[str] = None
    
    @validator('title')
    def validate_title(cls, v):
        if not v or len(v.strip()) < 5:
            raise ValueError('Title must be at least 5 characters')
        if len(v) > 200:
            raise ValueError('Title must be less than 200 characters')
        # Remove potentially dangerous characters
        return re.sub(r'[<>"\']', '', v.strip())
    
    @validator('description')
    def validate_description(cls, v):
        if not v or len(v.strip()) < 20:
            raise ValueError('Description must be at least 20 characters')
        if len(v) > 5000:
            raise ValueError('Description must be less than 5000 characters')
        return v.strip()
    
    @validator('acceptance_criteria')
    def validate_acceptance_criteria(cls, v):
        if not v or len(v) == 0:
            raise ValueError('At least one acceptance criterion is required')
        if len(v) > 20:
            raise ValueError('Maximum 20 acceptance criteria allowed')
        
        validated_criteria = []
        for criterion in v:
            if len(criterion.strip()) < 10:
                raise ValueError('Each acceptance criterion must be at least 10 characters')
            validated_criteria.append(criterion.strip())
        
        return validated_criteria
```

## Error Handling Patterns

### **Comprehensive Error Handling**

```python
# src/core/error_handling.py
from enum import Enum
from typing import Optional, Dict, Any
import traceback

class ErrorCode(Enum):
    """Standardized error codes"""
    VALIDATION_ERROR = "VALIDATION_ERROR"
    AGENT_EXECUTION_FAILED = "AGENT_EXECUTION_FAILED"
    LLM_API_ERROR = "LLM_API_ERROR"
    DATABASE_ERROR = "DATABASE_ERROR"
    HUMAN_VALIDATION_TIMEOUT = "HUMAN_VALIDATION_TIMEOUT"
    INSUFFICIENT_PERMISSIONS = "INSUFFICIENT_PERMISSIONS"
    RATE_LIMIT_EXCEEDED = "RATE_LIMIT_EXCEEDED"

class PathBridgeException(Exception):
    """Base exception for PathBridge errors"""
    
    def __init__(self, 
                 message: str, 
                 error_code: ErrorCode,
                 details: Optional[Dict[str, Any]] = None,
                 cause: Optional[Exception] = None):
        super().__init__(message)
        self.message = message
        self.error_code = error_code
        self.details = details or {}
        self.cause = cause
        self.timestamp = datetime.utcnow()
        self.traceback = traceback.format_exc() if cause else None

class ErrorHandler:
    """Centralized error handling"""
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
    
    async def handle_agent_error(self, 
                                agent_id: str, 
                                capability: str, 
                                error: Exception) -> CapabilityResponse:
        """Handle agent execution errors"""
        
        # Log the error with context
        self.logger.error(
            f"Agent {agent_id} failed executing {capability}",
            extra={
                'agent_id': agent_id,
                'capability': capability,
                'error_type': type(error).__name__,
                'error_message': str(error)
            }
        )
        
        # Determine error code
        if isinstance(error, ValidationError):
            error_code = ErrorCode.VALIDATION_ERROR
        elif isinstance(error, LLMAPIError):
            error_code = ErrorCode.LLM_API_ERROR
        else:
            error_code = ErrorCode.AGENT_EXECUTION_FAILED
        
        # Create structured error response
        return CapabilityResponse(
            success=False,
            result={},
            errors=[str(error)],
            warnings=[],
            execution_time_ms=0,
            metadata={
                'agent_id': agent_id,
                'error_code': error_code.value,
                'recoverable': self._is_recoverable_error(error)
            }
        )
    
    def _is_recoverable_error(self, error: Exception) -> bool:
        """Determine if error is recoverable"""
        recoverable_errors = (
            ConnectionError,
            TimeoutError,
            RateLimitError
        )
        return isinstance(error, recoverable_errors)
```

### **Retry Logic Implementation**

```python
# src/core/retry_logic.py
import asyncio
from typing import Callable, Any
import random

class RetryManager:
    """Manages retry logic with exponential backoff"""
    
    def __init__(self, 
                 max_retries: int = 3,
                 base_delay: float = 1.0,
                 max_delay: float = 60.0,
                 exponential_base: float = 2.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
    
    async def retry_with_backoff(self, 
                               func: Callable,
                               *args,
                               **kwargs) -> Any:
        """Execute function with exponential backoff retry"""
        
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    # Final attempt failed
                    break
                
                if not self._should_retry(e):
                    # Non-retryable error
                    break
                
                # Calculate delay with jitter
                delay = min(
                    self.base_delay * (self.exponential_base ** attempt),
                    self.max_delay
                )
                jitter = random.uniform(0.1, 0.3) * delay
                total_delay = delay + jitter
                
                await asyncio.sleep(total_delay)
        
        # All retries failed
        raise last_exception
    
    def _should_retry(self, error: Exception) -> bool:
        """Determine if error should trigger retry"""
        retryable_errors = (
            ConnectionError,
            TimeoutError,
            asyncio.TimeoutError,
            # Add LLM-specific retryable errors
        )
        return isinstance(error, retryable_errors)
```

## Configuration Management

### **Environment-Based Configuration**

```python
# src/config/settings.py
from pydantic import BaseSettings, Field
from typing import List, Optional
import os

class DatabaseSettings(BaseSettings):
    """Database configuration"""
    url: str = Field(..., env="DATABASE_URL")
    pool_size: int = Field(10, env="DB_POOL_SIZE")
    max_overflow: int = Field(20, env="DB_MAX_OVERFLOW")
    echo: bool = Field(False, env="DB_ECHO")

class LLMSettings(BaseSettings):
    """LLM provider configuration"""
    openai_api_key: Optional[str] = Field(None, env="OPENAI_API_KEY")
    anthropic_api_key: Optional[str] = Field(None, env="ANTHROPIC_API_KEY")
    default_model: str = Field("gpt-4", env="DEFAULT_LLM_MODEL")
    max_tokens: int = Field(4000, env="LLM_MAX_TOKENS")
    temperature: float = Field(0.1, env="LLM_TEMPERATURE")

class SecuritySettings(BaseSettings):
    """Security configuration"""
    jwt_secret_key: str = Field(..., env="JWT_SECRET_KEY")
    jwt_algorithm: str = Field("HS256", env="JWT_ALGORITHM")
    jwt_expiration_hours: int = Field(24, env="JWT_EXPIRATION_HOURS")
    allowed_origins: List[str] = Field(["*"], env="ALLOWED_ORIGINS")

class PathBridgeSettings(BaseSettings):
    """Main application settings"""
    environment: str = Field("development", env="ENVIRONMENT")
    debug: bool = Field(False, env="DEBUG")
    log_level: str = Field("INFO", env="LOG_LEVEL")
    
    # Component settings
    database: DatabaseSettings = DatabaseSettings()
    llm: LLMSettings = LLMSettings()
    security: SecuritySettings = SecuritySettings()
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

# Global settings instance
settings = PathBridgeSettings()
```

### **Feature Flags System**

```python
# src/config/feature_flags.py
from typing import Dict, Any
import json
import os

class FeatureFlagManager:
    """Manages feature flags for gradual rollouts"""
    
    def __init__(self):
        self.flags = self._load_flags()
    
    def _load_flags(self) -> Dict[str, Any]:
        """Load feature flags from configuration"""
        flags_file = os.getenv("FEATURE_FLAGS_FILE", "config/feature_flags.json")
        
        try:
            with open(flags_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return self._default_flags()
    
    def _default_flags(self) -> Dict[str, Any]:
        """Default feature flags"""
        return {
            "advanced_code_generation": {
                "enabled": True,
                "rollout_percentage": 100
            },
            "human_validation_gates": {
                "enabled": True,
                "rollout_percentage": 100
            },
            "parallel_agent_execution": {
                "enabled": False,
                "rollout_percentage": 0
            },
            "caching_enabled": {
                "enabled": True,
                "rollout_percentage": 50
            }
        }
    
    def is_enabled(self, flag_name: str, user_id: str = None) -> bool:
        """Check if feature flag is enabled for user"""
        flag = self.flags.get(flag_name, {})
        
        if not flag.get("enabled", False):
            return False
        
        rollout_percentage = flag.get("rollout_percentage", 0)
        
        if rollout_percentage >= 100:
            return True
        
        if rollout_percentage <= 0:
            return False
        
        # Use user_id for consistent rollout
        if user_id:
            user_hash = hash(user_id) % 100
            return user_hash < rollout_percentage
        
        return False

# Global feature flag manager
feature_flags = FeatureFlagManager()
```

## Integration Examples

### **LLM Provider Integration**

For complete LLM integration details, see the [PathBridge LLM Integration Guide](pathbridge_llm_integration_guide.md).

### **RAG System Integration**

For vector database and knowledge retrieval integration, see the [PathBridge RAG Integration Guide](pathbridge_rag_integration_guide.md).

### **Testing Framework Integration**

For automated test generation and TDD methodology, see the [PathBridge Testing Integration Guide](pathbridge_testing_integration_guide.md).

### **File Operations Integration**

For intelligent file system management and code generation, see the [PathBridge File Operations Guide](pathbridge_file_operations_guide.md).

### **Human Validation Integration**

For decision approval workflows and human-AI collaboration, see the [PathBridge Human Validation Guide](pathbridge_human_validation_guide.md).

### **Security Integration**

For comprehensive security analysis and vulnerability management, see the [PathBridge Security Integration Guide](pathbridge_security_integration_guide.md).

**Quick Integration Example:**
```python
from path_framework.core.llm_client import get_llm_client, LLMRequest

class CustomAgent(CoreAgent):
    def __init__(self, config=None):
        super().__init__("custom_agent", phase=1, capabilities=["analysis"])
        self.llm_client = get_llm_client()  # Uses environment configuration
    
    async def execute_capability(self, request: CapabilityRequest) -> CapabilityResponse:
        llm_request = LLMRequest(
            prompt=request.parameters.get("prompt"),
            system_prompt="You are an expert software architect",
            temperature=0.1,
            max_tokens=2000
        )
        
        response = await self.llm_client.generate(llm_request)
        
        return CapabilityResponse(
            success=True,
            result={"analysis": response.content},
            errors=[],
            warnings=[],
            execution_time_ms=response.tokens_used,
            metadata={"model_used": response.model_used}
        )
```

### **IDE Plugin Integration**

```typescript
// vscode-extension/src/extension.ts
import * as vscode from 'vscode';
import { PathBridgeClient } from './pathbridge-client';

export function activate(context: vscode.ExtensionContext) {
    const client = new PathBridgeClient();
    
    // Register command for user story execution
    const executeUserStory = vscode.commands.registerCommand(
        'pathbridge.executeUserStory',
        async () => {
            const editor = vscode.window.activeTextEditor;
            if (!editor) return;
            
            // Get user story from selection or input
            const userStory = await getUserStoryInput();
            
            // Show progress
            await vscode.window.withProgress({
                location: vscode.ProgressLocation.Notification,
                title: "PathBridge: Executing User Story",
                cancellable: true
            }, async (progress, token) => {
                
                const result = await client.executeUserStory(userStory, {
                    onProgress: (phase, status) => {
                        progress.report({
                            message: `Phase ${phase}: ${status}`
                        });
                    }
                });
                
                if (result.success) {
                    // Apply generated code
                    await applyGeneratedCode(result.artifacts);
                    vscode.window.showInformationMessage(
                        'User story executed successfully!'
                    );
                } else {
                    vscode.window.showErrorMessage(
                        `Execution failed: ${result.error}`
                    );
                }
            });
        }
    );
    
    context.subscriptions.push(executeUserStory);
}

class PathBridgeClient {
    private baseUrl: string;
    
    constructor() {
        this.baseUrl = vscode.workspace.getConfiguration('pathbridge')
            .get('apiUrl', 'http://localhost:8000');
    }
    
    async executeUserStory(userStory: any, callbacks?: any): Promise<any> {
        const response = await fetch(`${this.baseUrl}/api/v1/user-stories/execute`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${await this.getAuthToken()}`
            },
            body: JSON.stringify(userStory)
        });
        
        return response.json();
    }
}
```

### **CI/CD Pipeline Integration**

```yaml
# .github/workflows/pathbridge-development.yml
name: PathBridge AI Development

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  ai-development:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup PathBridge
      uses: pathbridge/setup-action@v1
      with:
        api-key: ${{ secrets.PATHBRIDGE_API_KEY }}
        
    - name: Extract User Story from Commit
      id: extract-story
      run: |
        # Extract user story from commit message or PR description
        STORY=$(echo "${{ github.event.head_commit.message }}" | grep -E "As a.*I want.*So that" || echo "")
        echo "story=$STORY" >> $GITHUB_OUTPUT
        
    - name: Execute PathBridge Development
      if: steps.extract-story.outputs.story != ''
      uses: pathbridge/execute-action@v1
      with:
        user-story: ${{ steps.extract-story.outputs.story }}
        auto-commit: true
        require-human-approval: ${{ github.ref == 'refs/heads/main' }}
        
    - name: Run Generated Tests
      run: |
        pytest tests/ --cov=src --cov-report=xml
        
    - name: Deploy to Staging
      if: github.ref == 'refs/heads/develop'
      run: |
        # Deploy generated artifacts to staging environment
        ./scripts/deploy-staging.sh
```

## Performance Metrics & Monitoring

### **Business Metrics Dashboard**

```python
# src/monitoring/business_metrics.py
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List

@dataclass
class BusinessMetrics:
    """Business performance metrics"""
    development_velocity: float  # Stories per week
    time_to_market: float       # Days from story to production
    defect_rate: float          # Defects per story
    user_satisfaction: float    # 1-5 rating
    roi_percentage: float       # Return on investment
    cost_savings: float         # Dollar amount saved

class BusinessMetricsCollector:
    """Collects and calculates business metrics"""
    
    async def calculate_development_velocity(self, 
                                           start_date: datetime,
                                           end_date: datetime) -> float:
        """Calculate stories completed per week"""
        completed_stories = await self.db.get_completed_stories(start_date, end_date)
        weeks = (end_date - start_date).days / 7
        return len(completed_stories) / weeks if weeks > 0 else 0
    
    async def calculate_time_to_market(self) -> float:
        """Calculate average time from story creation to production"""
        recent_stories = await self.db.get_recent_completed_stories(days=30)
        
        total_time = 0
        for story in recent_stories:
            time_diff = story.deployed_at - story.created_at
            total_time += time_diff.total_seconds()
        
        if len(recent_stories) == 0:
            return 0
        
        avg_seconds = total_time / len(recent_stories)
        return avg_seconds / (24 * 3600)  # Convert to days
    
    async def calculate_roi(self) -> Dict[str, float]:
        """Calculate ROI metrics"""
        # Implementation cost (development time * hourly rate)
        dev_hours = await self.get_development_hours()
        hourly_rate = 150  # Average developer hourly rate
        implementation_cost = dev_hours * hourly_rate
        
        # Benefits (time saved, defect reduction, faster delivery)
        time_savings = await self.calculate_time_savings()
        defect_savings = await self.calculate_defect_savings()
        delivery_savings = await self.calculate_delivery_savings()
        
        total_benefits = time_savings + defect_savings + delivery_savings
        roi_percentage = ((total_benefits - implementation_cost) / implementation_cost) * 100
        
        return {
            "implementation_cost": implementation_cost,
            "total_benefits": total_benefits,
            "roi_percentage": roi_percentage,
            "payback_period_months": implementation_cost / (total_benefits / 12)
        }
```

### **Quality Metrics Tracking**

```python
# src/monitoring/quality_metrics.py
class QualityMetricsTracker:
    """Tracks code quality and testing metrics"""
    
    async def track_test_coverage(self, story_id: str) -> Dict[str, float]:
        """Track test coverage for generated code"""
        coverage_data = await self.run_coverage_analysis(story_id)
        
        return {
            "line_coverage": coverage_data.get("line_coverage", 0),
            "branch_coverage": coverage_data.get("branch_coverage", 0),
            "function_coverage": coverage_data.get("function_coverage", 0),
            "mutation_score": await self.run_mutation_testing(story_id)
        }
    
    async def track_code_quality(self, story_id: str) -> Dict[str, Any]:
        """Track code quality metrics"""
        quality_data = await self.analyze_code_quality(story_id)
        
        return {
            "cyclomatic_complexity": quality_data.get("complexity", 0),
            "maintainability_index": quality_data.get("maintainability", 0),
            "technical_debt_ratio": quality_data.get("debt_ratio", 0),
            "code_smells": quality_data.get("smells", []),
            "security_vulnerabilities": quality_data.get("vulnerabilities", [])
        }
    
    async def generate_quality_report(self, story_id: str) -> Dict[str, Any]:
        """Generate comprehensive quality report"""
        test_metrics = await self.track_test_coverage(story_id)
        quality_metrics = await self.track_code_quality(story_id)
        
        # Calculate overall quality score
        quality_score = self._calculate_quality_score(test_metrics, quality_metrics)
        
        return {
            "story_id": story_id,
            "quality_score": quality_score,
            "test_metrics": test_metrics,
            "quality_metrics": quality_metrics,
            "recommendations": self._generate_recommendations(quality_metrics),
            "generated_at": datetime.utcnow().isoformat()
        }
```

## Troubleshooting Guide

### **Common Issues & Solutions**

#### **Agent Execution Failures**

```python
# Diagnostic script for agent issues
async def diagnose_agent_failure(agent_id: str, capability: str, request_data: Dict):
    """Diagnose agent execution failures"""
    
    print(f"Diagnosing {agent_id} - {capability}")
    
    # Check agent registration
    if agent_id not in orchestrator.agents:
        print(f" Agent {agent_id} not registered")
        return
    
    agent = orchestrator.agents[agent_id]
    
    # Check capability availability
    if capability not in agent.capabilities:
        print(f" Capability {capability} not available")
        print(f"Available capabilities: {agent.capabilities}")
        return
    
    # Check LLM connectivity
    try:
        test_response = await agent.llm_client.generate_completion(
            prompt="Test connection",
            max_tokens=10
        )
        print(" LLM connection working")
    except Exception as e:
        print(f" LLM connection failed: {str(e)}")
        return
    
    # Check knowledge base connectivity
    try:
        await agent.knowledge_base.test_connection()
        print(" Knowledge base connection working")
    except Exception as e:
        print(f" Knowledge base connection failed: {str(e)}")
        return
    
    # Test capability execution with minimal data
    try:
        test_request = CapabilityRequest(
            capability_name=capability,
            parameters={"test": True},
            context={},
            user_story_id="test"
        )
        
        response = await agent.execute_capability(test_request)
        if response.success:
            print(" Capability execution working")
        else:
            print(f" Capability execution failed: {response.errors}")
    except Exception as e:
        print(f" Capability execution error: {str(e)}")
```

#### **Performance Issues**

```python
# Performance diagnostic tools
class PerformanceDiagnostics:
    """Tools for diagnosing performance issues"""
    
    async def profile_agent_execution(self, agent_id: str, capability: str):
        """Profile agent execution performance"""
        import cProfile
        import pstats
        
        profiler = cProfile.Profile()
        
        # Profile the execution
        profiler.enable()
        # ... execute agent capability
        profiler.disable()
        
        # Analyze results
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # Top 20 functions
    
    async def analyze_database_performance(self):
        """Analyze database query performance"""
        slow_queries = await self.db.get_slow_queries()
        
        for query in slow_queries:
            print(f"Slow query: {query['query']}")
            print(f"Execution time: {query['duration']}ms")
            print(f"Suggestions: {self._suggest_query_optimizations(query)}")
    
    async def check_memory_usage(self):
        """Check memory usage patterns"""
        import psutil
        import gc
        
        process = psutil.Process()
        memory_info = process.memory_info()
        
        print(f"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")
        print(f"Memory percent: {process.memory_percent():.2f}%")
        
        # Check for memory leaks
        gc.collect()
        print(f"Objects in memory: {len(gc.get_objects())}")
```

## Best Practices

### **Development Best Practices**

1. **Test-Driven Development**
   - Write tests before implementing agent capabilities
   - Maintain >90% test coverage
   - Use mutation testing to validate test quality

2. **Error Handling**
   - Always handle exceptions gracefully
   - Provide meaningful error messages
   - Implement retry logic for transient failures

3. **Performance Optimization**
   - Use async/await for I/O operations
   - Implement caching for expensive operations
   - Profile code regularly to identify bottlenecks

4. **Security**
   - Validate all inputs
   - Use parameterized queries
   - Implement proper authentication and authorization

### **Deployment Best Practices**

1. **Environment Management**
   - Use environment-specific configurations
   - Implement feature flags for gradual rollouts
   - Monitor application health continuously

2. **Scaling Strategies**
   - Design for horizontal scaling
   - Use load balancing for high availability
   - Implement circuit breakers for external dependencies

3. **Monitoring & Observability**
   - Implement structured logging
   - Track business and technical metrics
   - Set up alerting for critical issues

## Getting Started

### **Quick Setup Guide**

```bash
# 1. Clone the repository
git clone https://github.com/pathbridge/ai-assistant.git
cd ai-assistant

# 2. Setup environment
cp .env.example .env
# Edit .env with your configuration

# 3. Install dependencies
uv sync

# 4. Setup database
docker-compose up -d postgres redis
alembic upgrade head

# 5. Run the application
uvicorn src.main:app --reload

# 6. Test the installation
curl http://localhost:8000/health
```

### **First User Story Example**

```python
# example_usage.py
import asyncio
from pathbridge.orchestration.agent_orchestrator import AgentOrchestrator

async def main():
    orchestrator = AgentOrchestrator()
    
    user_story = {
        "title": "User Authentication API",
        "description": "As a developer, I want to create a user authentication API so that users can securely log into the application",
        "acceptance_criteria": [
            "Given valid credentials, when I call POST /auth/login, then I receive a JWT token",
            "Given invalid credentials, when I call POST /auth/login, then I receive a 401 error",
            "Given a valid JWT token, when I call protected endpoints, then I can access them",
            "Given an expired JWT token, when I call protected endpoints, then I receive a 401 error"
        ],
        "business_value": "Enable secure user access to the application"
    }
    
    result = await orchestrator.execute_user_story(user_story)
    
    if result["success"]:
        print(" User story executed successfully!")
        print(f"Generated files: {len(result['artifacts'])}")
        print(f"Test coverage: {result['metrics']['test_coverage']}%")
    else:
        print(f" Execution failed: {result['error']}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

This implementation guide provides the complete technical foundation for building the PathBridge AI Coding Assistant. It covers all aspects from architecture design to deployment, ensuring developers have everything needed to implement the system successfully.